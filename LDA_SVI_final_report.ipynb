{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cd37476",
   "metadata": {},
   "source": [
    "# üìö Latent Dirichlet Allocation via Stochastic Variational Inference\n",
    "\n",
    "_Implementation and application of the algorithm proposed in \"Stochastic Variational Inference\" (Hoffman et al., 2013) for topic modeling on a Wikipedia-based corpus._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddccc80",
   "metadata": {},
   "source": [
    "## üß≠ Introduction\n",
    "\n",
    "This project is based on the seminal paper [_Stochastic Variational Inference_](https://www.jmlr.org/papers/volume14/hoffman13a/hoffman13a.pdf) by Hoffman, Blei, Wang and Paisley (2013), which introduces a scalable alternative to classical variational inference by leveraging stochastic optimization techniques.\n",
    "\n",
    "Our goal is to implement from scratch the algorithm proposed in the article and apply it to a real-world problem: unsupervised topic discovery from a corpus of Wikipedia articles. We focus on **Latent Dirichlet Allocation (LDA)** as the probabilistic model to which we apply **Stochastic Variational Inference (SVI)**.\n",
    "\n",
    "This notebook will walk through the theoretical foundations, the practical implementation, and the analysis of the resulting topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014fcc6f",
   "metadata": {},
   "source": [
    "## üß± Notebook Structure\n",
    "\n",
    "1. [Overview of Variational Inference](#variational)\n",
    "2. [Motivation for Stochastic Variational Inference](#svi)\n",
    "3. [Summary of the Algorithm in the Article](#algo-summary)\n",
    "4. [Our Approach and Data Preparation](#our-approach)\n",
    "5. [Latent Dirichlet Allocation (LDA)](#lda)\n",
    "6. [SVI Algorithm for LDA](#svi-lda)\n",
    "7. [Model Training and Hyperparameter Choices](#training)\n",
    "8. [Results Visualization and Interpretation](#results)\n",
    "9. [Discussion](#discussion)\n",
    "10. [Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1870a74d",
   "metadata": {},
   "source": [
    "## üìò Overview of Variational Inference\n",
    "\n",
    "Variational Inference (VI) is a deterministic technique used to approximate intractable posterior distributions in Bayesian models. Rather than using sampling (like MCMC), VI turns inference into an optimization problem: it seeks the closest distribution (within a tractable family) to the true posterior by minimizing the Kullback‚ÄìLeibler (KL) divergence.\n",
    "\n",
    "Mathematically, given a model with observed variables $x$ and latent variables $z$, and a prior $p(z)$, VI introduces a variational distribution $q(z; \\lambda)$ and minimizes:\n",
    "\n",
    "$$\n",
    "\\text{KL}(q(z; \\lambda) \\| p(z \\mid x)) \\quad \\Longleftrightarrow \\quad \\text{maximize } \\mathcal{L}(q) = \\mathbb{E}_q[\\log p(x, z)] - \\mathbb{E}_q[\\log q(z)]\n",
    "$$\n",
    "\n",
    "This objective, $\\mathcal{L}(q)$, is known as the **Evidence Lower Bound (ELBO)**. It's maximized when $q(z) \\approx p(z \\mid x)$.\n",
    "\n",
    "**Applications**:\n",
    "- Topic modeling (LDA)\n",
    "- Bayesian neural networks\n",
    "- Hidden Markov Models\n",
    "- Probabilistic matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b67d14",
   "metadata": {},
   "source": [
    "## ‚ö° Why Stochastic Variational Inference?\n",
    "\n",
    "Classical variational inference methods require full passes over the data to optimize the ELBO, making them computationally expensive and non-scalable for large datasets.\n",
    "\n",
    "Hoffman et al. (2013) propose **Stochastic Variational Inference (SVI)** as a solution to this issue. SVI leverages mini-batch optimization and stochastic gradients to update the global variational parameters using small subsets of data.\n",
    "\n",
    "This makes it particularly suited to:\n",
    "- Streaming or online learning\n",
    "- Large-scale text corpora\n",
    "- Models with complex posterior structures\n",
    "\n",
    "SVI has since become foundational in scalable Bayesian inference and is widely adopted in modern probabilistic programming libraries (e.g., Pyro, TensorFlow Probability)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d79ad26",
   "metadata": {},
   "source": [
    "## üîÅ Summary of the Algorithm from Hoffman et al. (2013)\n",
    "\n",
    "The algorithm alternates between:\n",
    "1. **Local inference**: Estimating document-specific variational parameters $\\phi$, $\\gamma$\n",
    "2. **Global update**: Updating the global parameters $\\lambda$ via a learning rate $\\rho_t$\n",
    "\n",
    "Each iteration samples a document and performs:\n",
    "- An E-step: variational updates of $\\phi$ and $\\gamma$\n",
    "- An M-step: update of $\\lambda$ via:\n",
    "\n",
    "$$\n",
    "\\lambda^{(t)} = (1 - \\rho_t)\\lambda^{(t-1)} + \\rho_t \\hat{\\lambda}\n",
    "$$\n",
    "\n",
    "Where $\\rho_t = (\\tau + t)^{-\\kappa}$ is the learning rate and $\\hat{\\lambda}$ is a scaled intermediate estimate based on the sampled document.\n",
    "\n",
    "This stochastic approximation converges under mild conditions and enables inference on massive datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4c93f8",
   "metadata": {},
   "source": [
    "## üß© Our Setup and Corpus\n",
    "\n",
    "To apply the SVI algorithm to a concrete task, we chose **Latent Dirichlet Allocation (LDA)** on a corpus of **~1200 Wikipedia articles**, structured by 8 distinct themes (seeds) such as History, Art, Science, and Politics.\n",
    "\n",
    "Each article was fetched using the `wikipedia-api` library, then preprocessed:\n",
    "- Tokenization and normalization\n",
    "- Stopword removal\n",
    "- Vocabulary construction\n",
    "- Bag-of-Words encoding\n",
    "\n",
    "This yields a corpus ready for topic modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d82f31",
   "metadata": {},
   "source": [
    "## üìö Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "LDA is a generative probabilistic model where:\n",
    "- Each document is modeled as a mixture of topics\n",
    "- Each topic is a distribution over words\n",
    "\n",
    "### Generative process:\n",
    "1. For each topic $k$, draw $\\beta_k \\sim \\text{Dir}(\\eta)$\n",
    "2. For each document $d$:\n",
    "    - Draw $\\theta_d \\sim \\text{Dir}(\\alpha)$\n",
    "    - For each word $n$:\n",
    "        - Draw topic assignment $z_{dn} \\sim \\text{Mult}(\\theta_d)$\n",
    "        - Draw word $w_{dn} \\sim \\text{Mult}(\\beta_{z_{dn}})$\n",
    "\n",
    "This model is widely used for unsupervised discovery of thematic structures in text corpora.\n",
    "\n",
    "We model $\\theta$ and $\\beta$ using variational approximations with Dirichlet distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c641830e",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è SVI for LDA ‚Äî Our Implementation\n",
    "\n",
    "We implement the algorithm following the exact procedure from Hoffman et al.:\n",
    "- Initialization of global parameters $\\lambda \\sim \\text{Gamma}$\n",
    "- Per-document E-step: update $\\phi$, $\\gamma$\n",
    "- Global M-step: update $\\lambda$ via a decaying learning rate\n",
    "\n",
    "Our implementation is modular and cleanly separates core steps.\n",
    "\n",
    "```python\n",
    "# The function SVI_for_LDA(...) is inserted here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5522d767",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üü© **Training and Parameter Choices** <a name=\"training\"></a>\n",
    "\n",
    "```markdown\n",
    "## ‚öôÔ∏è Training and Hyperparameter Choices\n",
    "\n",
    "To train the model, we used the following configuration:\n",
    "\n",
    "- $K = 10$ topics\n",
    "- $\\alpha = 1.0$ (Dirichlet prior for document-topic)\n",
    "- $\\eta = 1.0$ (Dirichlet prior for topic-word)\n",
    "- $\\tau = 100$, $\\kappa = 0.7$ (controls learning rate decay)\n",
    "- $max\\_iter = 500$, $e\\_step\\_iter = 30$\n",
    "\n",
    "These values were chosen based on:\n",
    "- Recommendations from the original paper\n",
    "- Common defaults in the literature\n",
    "- Preliminary testing on the dataset size (~1200 documents)\n",
    "\n",
    "```python\n",
    "# Insert SVI model training cell here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3e07c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### üü© **Visualizing and Analyzing the Results** <a name=\"results\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c009511",
   "metadata": {},
   "source": [
    "## üìä Results and Visualizations\n",
    "\n",
    "We analyze three key aspects of the learned model:\n",
    "1. __Topic-word distributions__ ‚Äî the top words most strongly associated with each topic.\n",
    "2. **Document-topic distributions** ‚Äî how each article is distributed over topics.\n",
    "3. **Seed-topic associations** ‚Äî how the original seed themes align with the inferred topics.\n",
    "\n",
    "Visualizations include:\n",
    "- Word clouds per topic\n",
    "- Pie chart of dominant topic frequencies\n",
    "- Stacked bar chart showing topic distribution by seed\n",
    "- Heatmaps of topic-seed associations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb519f9",
   "metadata": {},
   "source": [
    "## üí¨ Discussion\n",
    "\n",
    "The results show clear patterns of topic specialization across the corpus. Some topics align closely with expected themes (e.g., a topic with \"mus√©e\", \"peinture\", \"artiste\" aligns well with the 'Art' seed).\n",
    "\n",
    "However, a few topics dominate disproportionately, which suggests:\n",
    "- Possible imbalance in the corpus\n",
    "- Overlapping vocabulary between themes\n",
    "- A need to tune $\\alpha$, $K$, or consider topic coherence metrics\n",
    "\n",
    "Further experiments (e.g., increasing topic count, or using seed-supervision) could help refine topic separation and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ab782d",
   "metadata": {},
   "source": [
    "## üßæ Conclusion\n",
    "\n",
    "This project demonstrated a full implementation of **Stochastic Variational Inference** as proposed in Hoffman et al. (2013), applied to unsupervised topic modeling with LDA.\n",
    "\n",
    "We:\n",
    "- Built a Wikipedia corpus of ~1200 articles across 8 themes\n",
    "- Designed and implemented a custom SVI algorithm\n",
    "- Trained and analyzed an interpretable LDA model\n",
    "- Visualized the discovered structure across documents and themes\n",
    "\n",
    "### üîç Improvements & Extensions:\n",
    "- Try dynamic topic models (DTM)\n",
    "- Explore topic coherence metrics (`c_v`, `NPMI`)\n",
    "- Use more advanced corpora and larger datasets\n",
    "- Evaluate document clustering by topic assignments\n",
    "\n",
    "**SVI remains a powerful and scalable method** for modern probabilistic modeling, and this project showcases its potential for interpretable NLP pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1cb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import digamma\n",
    "\n",
    "from src import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b483d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wikipedia_corpus.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    wiki_articles = json.load(f)\n",
    "\n",
    "texts = [article[\"content\"] for article in wiki_articles.values()] # Extract the content of the articles\n",
    "titles = [article[\"title\"] for article in wiki_articles.values()] # Extracrt the title title of the articles\n",
    "corpus, vocab = vectorize_texts(texts) # Vectorized texts\n",
    "\n",
    "V = len(vocab) # Vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116cded9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_dirichlet_expectation(alpha):\n",
    "    \"\"\"\n",
    "    Computes the expectation of the log of a Dirichlet distribution.\n",
    "\n",
    "    Args:\n",
    "        alpha (np.ndarray): Dirichlet parameters, 1D or 2D array.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Expected log values of the Dirichlet-distributed variables.\n",
    "    \"\"\"\n",
    "    if len(alpha.shape) == 1:\n",
    "        return digamma(alpha) - digamma(np.sum(alpha))\n",
    "    return digamma(alpha) - digamma(np.sum(alpha, axis=1))[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d94df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVI_for_LDA(documents, V, K=10, alpha=1.0, eta=1.0, max_iter=100, tau=64, kappa=0.7, e_step_iter=20, verbose=2):\n",
    "    \"\"\"\n",
    "    Trains a Latent Dirichlet Allocation (LDA) model using Stochastic Variational Inference (SVI).\n",
    "\n",
    "    Args:\n",
    "        documents (list of list of int): Corpus as lists of word indices.\n",
    "        V (int): Vocabulary size.\n",
    "        K (int, optional): Number of topics. Defaults to 10.\n",
    "        alpha (float, optional): Dirichlet prior for document-topic distribution. Defaults to 1.0.\n",
    "        eta (float, optional): Dirichlet prior for topic-word distribution. Defaults to 1.0.\n",
    "        max_iter (int, optional): Number of training iterations. Defaults to 100.\n",
    "        tau (float, optional): Learning rate delay. Defaults to 64.\n",
    "        kappa (float, optional): Learning rate decay factor (0.5 < kappa ‚â§ 1). Defaults to 0.7.\n",
    "        e_step_iter (int, optional): Number of E-step iterations per document. Defaults to 20.\n",
    "        verbose (int, optional): Verbosity level (0=silent, 1=summary, 2=full). Defaults to 2.\n",
    "\n",
    "    Returns:\n",
    "        tuple:\n",
    "            - np.ndarray: Learned topic-word distribution matrix (lambda).\n",
    "            - dict: Mapping from document titles to topic distributions.\n",
    "    \"\"\"\n",
    "    D = len(documents)\n",
    "    lambd = np.random.gamma(100., 1./100., size=(K, V))\n",
    "\n",
    "    doc_topic_distrib = {}\n",
    "\n",
    "    start_training_time = time.time() \n",
    "    for t in range(max_iter):\n",
    "        if verbose >= 2:\n",
    "            start_iteration_time = time.time() \n",
    "\n",
    "        doc_id = np.random.randint(0, D)\n",
    "        doc = documents[doc_id]\n",
    "        N = len(doc)\n",
    "\n",
    "        gamma = np.ones(K)\n",
    "        phi = np.full(shape=(N, K), fill_value=1/K)\n",
    "\n",
    "        E_logbeta = log_dirichlet_expectation(lambd)\n",
    "        exp_E_logbeta = np.exp(E_logbeta)\n",
    "\n",
    "        for _ in range(e_step_iter):\n",
    "            E_logtheta = log_dirichlet_expectation(gamma)\n",
    "            exp_E_logtheta = np.exp(E_logtheta)\n",
    "\n",
    "            for n, w in enumerate(doc):\n",
    "                phi[n, :] = exp_E_logtheta * exp_E_logbeta[:, w]\n",
    "                phi[n, :] /= np.sum(phi[n, :])\n",
    "            \n",
    "            gamma = alpha + np.sum(phi, axis=0)\n",
    "\n",
    "            theta = gamma / np.sum(gamma)\n",
    "            doc_topic_distrib[titles[doc_id]] = theta\n",
    "\n",
    "        topic_word_contrib = np.zeros(shape=(K, V))\n",
    "        for n, w in enumerate(doc):\n",
    "            topic_word_contrib[:, w] += phi[n, :]\n",
    "\n",
    "        lambd_hat = eta + D*topic_word_contrib\n",
    "\n",
    "        rho = (t + tau) ** -kappa\n",
    "        lambd = (1 - rho) * lambd + rho * lambd_hat\n",
    "\n",
    "        if verbose >= 2:\n",
    "            iteration_time = time.time() - start_iteration_time\n",
    "            print(f\"\\t - Iteration {t} done in {iteration_time:.2f}s.\")\n",
    "\n",
    "    if verbose >= 1:\n",
    "        training_time = time.time() - start_training_time\n",
    "        print(f\"Training done in {training_time:.2f}s.\")\n",
    "\n",
    "    return lambd, doc_topic_distrib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118bc1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd, doc_topic_distrib = SVI_for_LDA(\n",
    "    corpus,\n",
    "    V,\n",
    "    K=10,\n",
    "    alpha=1.0,\n",
    "    eta=1.0,\n",
    "    max_iter=500,\n",
    "    tau=100,\n",
    "    kappa=0.7,\n",
    "    e_step_iter=30,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1a37ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_words = get_top_words(lambd, vocab)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936aad6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_top_words_wordcloud(lambd, vocab, n_wordcloud=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed37e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_info = get_topic_distrib_info(doc_topic_distrib, wiki_articles)\n",
    "docs_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899834aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_topics_distrib(docs_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bea58cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_by_seed = plot_topics_distrib_by_seed(docs_info)\n",
    "topics_by_seed"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

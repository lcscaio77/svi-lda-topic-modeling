{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìå 1. Pr√©sentation du contexte de l'article\n",
    "\n",
    "### üß† Probl√®me consid√©r√©\n",
    "\n",
    "L‚Äôarticle **‚ÄúStochastic Variational Inference‚Äù** de Hoffman, Blei, Wang et Paisley (2013) s‚Äôattaque √† un **probl√®me fondamental** en apprentissage bay√©sien : comment faire de l‚Äô**inf√©rence approch√©e dans des mod√®les probabilistes complexes**, quand le volume de donn√©es est √©norme.\n",
    "\n",
    "Ces mod√®les incluent des structures hi√©rarchiques avec :\n",
    "- **Variables latentes locales**, propres √† chaque donn√©e (ex. : proportions de sujets d‚Äôun document dans LDA),\n",
    "- **Param√®tres globaux**, partag√©s entre toutes les donn√©es (ex. : sujets globaux dans LDA).\n",
    "\n",
    "L‚Äôenjeu est de pouvoir **calculer la distribution post√©rieure** de ces variables, ce qui est g√©n√©ralement **intractable en pratique**.\n",
    "\n",
    "---\n",
    "\n",
    "### üì¶ Type de donn√©es concern√©es\n",
    "\n",
    "L‚Äôarticle se concentre sur des **donn√©es massives** o√π chaque observation suit une structure similaire :\n",
    "- Documents textuels (comme dans LDA),\n",
    "- S√©ries temporelles,\n",
    "- Graphes,\n",
    "- Donn√©es biologiques, etc.\n",
    "\n",
    "Le principal exemple √©tudi√© est le mod√®le **LDA (Latent Dirichlet Allocation)** appliqu√© √† un corpus scientifique contenant **plus d‚Äôun million de documents**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ùå Limites des approches existantes\n",
    "\n",
    "Les approches classiques d‚Äôinf√©rence bay√©sienne souffrent de **graves limitations** √† grande √©chelle :\n",
    "\n",
    "- **MCMC** (Monte Carlo par cha√Ænes de Markov) : pr√©cis mais **trop lent**, surtout pour des millions d‚Äôexemples.\n",
    "- **Inf√©rence variationnelle standard** (batch VI) : transforme l‚Äôinf√©rence en un probl√®me d‚Äôoptimisation, plus rapide que MCMC, mais encore **trop co√ªteuse** :\n",
    "  - N√©cessite de **parcourir l‚Äôint√©gralit√© des donn√©es** √† chaque it√©ration.\n",
    "  - Consomme beaucoup de m√©moire et est **difficilement parall√©lisable**.\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Objectif de l'article\n",
    "\n",
    "Proposer une m√©thode :\n",
    "- **Scalable** : qui fonctionne √† l‚Äô√©chelle de millions de donn√©es,\n",
    "- **Efficace** : bas√©e sur des **mini-batchs**,\n",
    "- Th√©oriquement solide : convergence prouv√©e,\n",
    "- Appliqu√©e √† des mod√®les de la **famille exponentielle conjugu√©e** (comme LDA).\n",
    "\n",
    "C‚Äôest dans ce contexte qu‚Äôintervient la m√©thode **Stochastic Variational Inference (SVI)**, qui combine inf√©rence variationnelle + descente de gradient stochastique pour un apprentissage **rapide et it√©ratif**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß† 2. M√©thode propos√©e : Stochastic Variational Inference (SVI)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.1 üéØ Objectif du cadre variationnel\n",
    "\n",
    "Dans les mod√®les bay√©siens hi√©rarchiques comme LDA, l‚Äôobjectif est de calculer la distribution post√©rieure :\n",
    "\n",
    "$$\n",
    "p(\\beta, z \\mid x) = \\frac{p(x, z, \\beta)}{p(x)}\n",
    "$$\n",
    "\n",
    "Mais le d√©nominateur $p(x) = \\int p(x, z, \\beta)\\,dz\\,d\\beta$ est incomputable dans les cas r√©els.\n",
    "\n",
    "On approxime cette post√©rieure par une distribution factoris√©e :\n",
    "\n",
    "$$\n",
    "q(z, \\beta) = q(\\beta \\mid \\lambda) \\prod_{n=1}^N q(z_n \\mid \\phi_n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 üí° Inf√©rence variationnelle classique\n",
    "\n",
    "On minimise la divergence de Kullback-Leibler :\n",
    "\n",
    "$$\n",
    "\\mathrm{KL}(q(z, \\beta) \\parallel p(z, \\beta \\mid x)) = \\mathbb{E}_q\\left[\\log \\frac{q(z, \\beta)}{p(z, \\beta \\mid x)}\\right]\n",
    "$$\n",
    "\n",
    "Ce qui revient √† maximiser l‚ÄôELBO :\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\lambda, \\phi) = \\mathbb{E}_q[\\log p(x, z, \\beta)] - \\mathbb{E}_q[\\log q(z, \\beta)]\n",
    "$$\n",
    "\n",
    "> *‚ÄúVariational inference turns the problem of posterior inference into one of optimization.‚Äù*  \n",
    "> *(Page 2, colonne de droite)*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.3 ‚ö†Ô∏è Limites de l‚Äôapproche batch\n",
    "\n",
    "Chaque mise √† jour des param√®tres globaux $\\lambda$ d√©pend de toutes les donn√©es ‚Üí trop co√ªteux pour de grands jeux.\n",
    "\n",
    "> *‚ÄúIn massive data sets, traditional variational inference is computationally intractable.‚Äù*  \n",
    "> *(Page 2, colonne de droite)*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.4 üí° Id√©e de SVI : optimiser l‚ÄôELBO par sous-√©chantillons\n",
    "\n",
    "SVI remplace le gradient exact de l‚ÄôELBO par une estimation issue d‚Äôun sous-√©chantillon.\n",
    "\n",
    "On r√©√©crit :\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\lambda) = \\sum_{i=1}^N \\mathcal{L}_i(\\lambda)\n",
    "$$\n",
    "\n",
    "Et on met √† jour $\\lambda$ par descente stochastique :\n",
    "\n",
    "$$\n",
    "\\lambda^{(t)} = \\lambda^{(t-1)} + \\rho_t \\cdot \\hat{\\nabla} \\mathcal{L}_i(\\lambda)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 2.5 üî¢ Structure des mod√®les conjugu√©s\n",
    "\n",
    "Le mod√®le doit √™tre de la forme exponentielle conjugu√©e :\n",
    "\n",
    "- Prior :\n",
    "\n",
    "$$\n",
    "p(\\beta) = h(\\beta)\\exp(\\eta^\\top t(\\beta) - a(\\eta))\n",
    "$$\n",
    "\n",
    "- Likelihood :\n",
    "\n",
    "$$\n",
    "p(x, z \\mid \\beta) = f(x, z)\\exp(t(\\beta)^\\top u(x, z))\n",
    "$$\n",
    "\n",
    "Cela permet des mises √† jour analytiques via les statistiques suffisantes.\n",
    "\n",
    "> *‚ÄúOur method applies to a large class of models with conjugate exponential families.‚Äù*  \n",
    "> *(Page 3, colonne de droite)*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.6 üìê Mise √† jour naturelle des param√®tres\n",
    "\n",
    "Mise √† jour avec le **gradient naturel** (plus stable) :\n",
    "\n",
    "$$\n",
    "\\lambda^{(t)} = (1 - \\rho_t) \\lambda^{(t-1)} + \\rho_t \\hat{\\lambda}_i\n",
    "$$\n",
    "\n",
    "> *‚ÄúWe adapt stochastic optimization to variational inference using the natural gradient of the variational objective.‚Äù*  \n",
    "> *(Page 5, colonne de gauche)*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.7 ‚öôÔ∏è Taux d‚Äôapprentissage\n",
    "\n",
    "$$\n",
    "\\rho_t = (t + \\tau)^{-\\kappa}\n",
    "$$\n",
    "\n",
    "avec $\\kappa \\in (0.5, 1]$, $\\tau > 0$\n",
    "\n",
    "> *‚ÄúThese conditions ensure the stochastic algorithm converges to a local optimum of the ELBO.‚Äù*  \n",
    "> *(Appendix A)*\n",
    "\n",
    "---\n",
    "\n",
    "### 2.8 üìò Application √† LDA\n",
    "\n",
    "- Variables locales : $\\theta_d$, $z_{dn}$\n",
    "- Param√®tres globaux : $\\beta_k$\n",
    "\n",
    "SVI permet de :\n",
    "\n",
    "- Tirer un document\n",
    "- Optimiser ses variables locales\n",
    "- Mettre √† jour les distributions de sujets $\\beta_k$\n",
    "\n",
    "> Voir Figure 2, Page 13 pour l‚Äôalgorithme complet\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Avantages de SVI\n",
    "\n",
    "| Aspect       | Avantage                                        |\n",
    "|--------------|-------------------------------------------------|\n",
    "| Vitesse      | Mises √† jour locales rapides                    |\n",
    "| M√©moire      | Pas besoin de charger tout le dataset           |\n",
    "| Scalabilit√©  | Compatible avec des millions de documents       |\n",
    "| Th√©orique    | Convergence d√©montr√©e sous conditions           |\n",
    "| Modulaire    | Fonctionne avec d'autres mod√®les hi√©rarchiques |\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
